# サマリ

# メモ
## wordembedding
- 単語のベクトル化と混合されがちだが、必ずしもイコールではない
## 単語のベクトル化
### one-hot vector
- 単語の数だけ、次元を持つベクトル（超高次元）
- 全ての単語に固有ベクトルを作れる
- 単語の意味や瀕死の比較は行いづらい
### Word2Vec
- 単語ごとのベクトルを100~1000の次元数のベクトルとして表現
- 有名な「man + royal = king, female + royal = queen」のような演算が可能
- 対義語には弱い（好きと嫌いが対義語、とわからない）
### コサイン類似度
- Word2Vecなどでは、ベクトルの成す内角の大きさが類似度として考えられる
- つまり、Cosをとって、−１〜１で類似度を表すことが可能

## Transformer
- 翻訳タスクで、Attentionという手法で、RNNやCNN以上の精度を叩き出した

## Attention
- 文章のベクトル表現に「重要度」を加味
    - 入力する文章全体からどの部分が重要なのかを計算
    - その重要度を使ってベクトル表現を作る
- 以下のような（RNNやCNNの課題だった問題を解決できる）特徴を持つ
    - 出力に対する影響度が文章全体で均一になる
    - 離れた単語間の依存関係を学習できる
    - 文章の長さに対する単語間の依存関係の計算コストが抑えられる
### Soft Attention
- 重要度を重みとして使って文章全体を重み付き平均としてベクトル表現にするAttention
### Hard Attention
- 最も重要度が高い部分だけを取り出してベクトル表現にするAttention