# サマリ

# メモ
## 1. 強化学習入門
### 1.1 強化学習とは
#### 1.1.1 機械学習の分類
- 教師あり学習
    - 目的: 正解ラベル付きのトレーニングデータからモデルを学習し、未知のデータに対して予想すること
- 教師なし学習
    - 目的: 正解ラベルのないデータや構造が不明なデータに対し、データの構造や関係性を見出すこと
- 強化学習
    - 目的: エージェントと環境が相互作用するような状況下で、もっとも最適な行動を発見すること
#### 1.1.2 強化学習というタスク
- 前提とする状況：
    - エージェントが、状態sにあるとして、環境に対して行動aを取る。その結果、環境はその行動の評価として報酬Rを返し、エージェントは次の状態s'に移行する。
    - 上記を繰り返し、行動を強化しながらタスクを進める
- 報酬は、即得られる即時報酬だけではなく、長期的な報酬（遅延報酬）も含めた「収益」全体を最大化することを目的とする必要がある。
    - 本質的な将来の価値を最大化することを目的とした問題は「強化学習に適したタスク」
- 不確実性のある状況において、真価を発揮する
    - ゲームやロボットの制御、ファイナンスなどに活用される
#### 1.1.3 N腕バンディット問題
- N腕バンディット問題とは:
    - 事前に当たる確率の定義されているスロットマシーンがN台ならんでいて、ユーザーはどのスロットマシーンがどのくらい当たるか知らされていない。
    - 一度スロットマシーンを引くと、それぞれ事前に設定されていた確率に基づき、当たりならば1、外れならば0という報酬が支払われる。ここでは簡単化のため、当たりの確率は変わらないものとする。
    - ユーザーは1回の試行につき、どれか1つのスロットマシーンを引ける。
- 試行回数あたりの平均報酬量を最大化するためにはどのようにするべきかを考える問題
    - エージェント自身はそれぞれの内部の確率を知らないために、実際に引くことによって得た報酬量からそれぞれの確率を推測しなければならないことが重要な点
#### 1.1.4 エージェントの作成
- エージェント: 環境の中で行動を決定し、環境に対して影響を与えるもの
    - N腕バンディット問題では、どのスロットマシーンを使用するかを判断し報酬を受け取り、次の判断をするユーザー
- 方策：取得した報酬から、どのようなアルゴリズムに基づいて次の腕を決めるかという指標のこと
#### 1.1.5 環境の作成
- 環境: エージェントが行動をおこす対象
    - エージェントの行動を受け、状況を観測し、報酬をエージェントに送信し、時間を1つ進めるという役割
    - N腕バンディッド問題においては、エージェントがあるスロットマシーンを引いた時、そのスロットの確率によって当たりか外れかを出すプロセス
#### 1.1.6 報酬の定義
- 報酬(reward): 環境からエージェントに与えられる信号のことで、エージェントの一連の行動の望ましさを評価する指標
    - N腕バンディッド問題において言えばスロットマシーンから得られた返り値
    - これは、即与えられるので「即時報酬」
#### 1.1.7 まとめ
- 配列の累積話を出す関数：`np.cumsum()`
- 累積和の遷移をみる方法: `np.cumsum(record) / np.arrange(1,record.size+1)`
    - 分子は累積和の配列（0までの累積、1までの累積, ... , nまでの累積)
    - 分母は(1,2,3, ... , n)
    - 累積和の平均の配列が求められる
- 実装例(引用):
```
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

# 手法を定義する関数です
def randomselect():
    slot_num = np.random.randint(0, 5)
    return slot_num

# 環境を定義する関数です
def environments(band_number):
    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])
    results = np.random.binomial(1, coins_p)
    result = results[band_number]
    return result

# 報酬を定義する関数です
def reward(record, results, slot_num, time):
    result = environments(slot_num)
    record[time - 1] = result
    results[slot_num][1] += 1
    results[slot_num][2] += result
    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]
    return results, record

    
# 初期変数を設定しています
times = 10000
results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]
record = np.zeros(times)

# slot_numを取得して、results,recordを書き換えてください
for time in range(0, times):
    slot_num = randomselect()
    results, record = reward(record, results, slot_num, time)

# 各マシーンの試行回数と結果を出力しています
print(results)

# recordを用いて平均報酬の推移をプロットしてください
plt.plot(np.cumsum(record) / np.arange(1, record.size + 1))

# 表を出力しています
plt.xlabel("試行回数")
plt.ylabel("平均報酬")
plt.title("試行回数と平均報酬の推移")
plt.show()
```
### 1.2 N腕バンディッド問題における方策
#### 1.2.1 greedy手法
- 最初はランダムに、ある程度情報が溜まれば最も良さそうなものを選び続ける
    - 良さそうなものを選び続けることを「利用」と呼ぶ
    - 試行回数が少ないうちは確率の偏りがあるため、一見成功率の低く見える腕も選択する(「探索」と呼ぶ)必要があある
- greedy手法
    - これまでの結果から最も期待値の大きいスロットマシーンを選択する手法
    - 探索：N腕を、n回ずつ動かす探索を行う
- 実装例（引用）
```
# results = [[腕の番号, 試行回数, 報酬合計, 報酬合計 / 試行回数], [腕の番号, 試行回数, 報酬合計, 報酬合計 / 試行回数], [], []...]
def greedy(results, n):
    # 試行回数がnより少ないマシンがある場合、slot_numをそのマシンの腕番号にする 
    slot_num = None
    for i, d in enumerate(results):
        if d[1] < n:
            slot_num = i
            break
        
    # どのマシンの試行回数もnより大きい場合、slot_numを報酬の期待値の高いものにする
    if slot_num == None:
        slot_num = np.argmax(np.array(results), axis = 0)[0]
        
    return slot_num
```
#### 1.2.2 ε-greedy手法
#### 1.2.3 楽観的初期値法
#### 1.2.4 soft-max法
#### 1.2.5 UCB1アルゴリズム
#### 1.2.6 まとめ
#### 1.2.7 探索と利用のトレードオフ
## 2. マルコフ決定過程とベルマン方程式
### 2.1 強化学習の構成要素
#### 2.1.1 強化学習の構成要素
#### 2.1.2 強化学習のモデル化
#### 2.1.3 状態・行動・報酬
### 2.2 マルコフ決定過程
#### 2.2.1 マルコフ性
#### 2.2.2 環境変化の記述-1
#### 2.2.3 環境変化の記述-2
#### 2.2.4 エピソード
### 2.3 価値・収益・状態
#### 2.3.1 報酬と収益
#### 2.3.2 価値関数・状態価値関数
### 2.4 最適な方策の探求
#### 2.4.1 最適方策
#### 2.4.2 行動価値
#### 2.4.3 最適状態価値関数・最適行動価値関数
### 2.5 ベルマン方程式
#### 2.5.1 最適な状態価値
#### 2.5.2 ベルマン方程式
#### 2.5.3 ベルマン最適方程式
## 3. 動的計画法とTD手法
### 3.1 動的計画法
#### 3.1.1 動的計画法とは
#### 3.1.2 方策評価
#### 3.1.3 方策反復
#### 3.1.4 価値反復
### 3.2 TD手法
#### 3.2.1 TD手法とは
#### 3.2.2 Sarsa
#### 3.2.3 SarsaにおけるQ関数の実装
#### 3.2.4 Sarsaでのε-greedy手法の実装
#### 3.2.5 Q学習