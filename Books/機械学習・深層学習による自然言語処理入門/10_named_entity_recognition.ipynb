{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_named_entity_recognition.ipynb","provenance":[{"file_id":"1HMbsNTR2ZGQ9K8D2wZINQKfefLAwDyFh","timestamp":1627915255979},{"file_id":"17QBibb081pFtgTmSnIpFV7Ixq3FzPiLH","timestamp":1579656913093}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7e70e4dbbaf048799bf4e163b78c2a36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_012132981b374dc0abbdc74d1775483d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_15ce6fff54644b6dae98535f561b6922","IPY_MODEL_7abfa0e91f2b4e39b16bdc457b427262"]}},"012132981b374dc0abbdc74d1775483d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15ce6fff54644b6dae98535f561b6922":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1b18548c2a02442b883155f9490e5e5a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":257706,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":257706,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6af325623a1d4cac99669e8cf29e6f57"}},"7abfa0e91f2b4e39b16bdc457b427262":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ff44f17e855643788d6d5c9fc66c36ac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 258k/258k [00:00&lt;00:00, 701kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6633b4bb9adb48acbb67eb1aa8fbb90b"}},"1b18548c2a02442b883155f9490e5e5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6af325623a1d4cac99669e8cf29e6f57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff44f17e855643788d6d5c9fc66c36ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6633b4bb9adb48acbb67eb1aa8fbb90b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4cc3b804bca34591bc55d4619158a914":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7858848c3dc0435bb47ae5ecf7e749e7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e62557cea3794df29fbf304c4b04258a","IPY_MODEL_f671eaaab82a433c903756671539ce11"]}},"7858848c3dc0435bb47ae5ecf7e749e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e62557cea3794df29fbf304c4b04258a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8bb09890ae2b49598643c833f4cdae3d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1b8f32dd61d34a3f84d6f72fedf4795f"}},"f671eaaab82a433c903756671539ce11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b906dc6aba949ff82a8ed310c216210","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:07&lt;00:00, 55.8B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87710c4eb3444a0aaf13ca70eece61c1"}},"8bb09890ae2b49598643c833f4cdae3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1b8f32dd61d34a3f84d6f72fedf4795f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b906dc6aba949ff82a8ed310c216210":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"87710c4eb3444a0aaf13ca70eece61c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a831c6c08135497097130e4d3edbbbd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7f91460ebceb46aa96e88841437763f5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0fbc1970fc894e8d943c5ad7a64d26e1","IPY_MODEL_d2bb35a5fc294888b2b603994f52f96a"]}},"7f91460ebceb46aa96e88841437763f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fbc1970fc894e8d943c5ad7a64d26e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2b0c454f81c44ffe9480eddd80f16fae","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":545149952,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":545149952,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc9bb739117445b8aa1588deb1062ccb"}},"d2bb35a5fc294888b2b603994f52f96a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_50aeeb7f00ac4bb388fa881a8f99b82f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 545M/545M [00:07&lt;00:00, 71.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f8731e1d3954a58b9748110e8789d32"}},"2b0c454f81c44ffe9480eddd80f16fae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc9bb739117445b8aa1588deb1062ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50aeeb7f00ac4bb388fa881a8f99b82f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3f8731e1d3954a58b9748110e8789d32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"hy7zVGAsDj7t"},"source":["# Named Entity Recognition using Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"A6kLbNONDFsQ"},"source":["## History\n","\n","### 2020/9/3\n","\n","I fixed some issues. Sorry for the inconvenience.\n","\n","- Change the name of pre-trained BERT model(bert-base-japanese-whole-word-masking -> cl-tohoku/bert-base-japanese-whole-word-masking)\n","- Update `evaluate` function due to the version upgrade of Transformers(v2.3.0 -> v3.1.0)\n","- Fix the version of transformers(v3.1.0)\n","- Reduce the `batch_size` from 32 to 16 due to OOM"]},{"cell_type":"markdown","metadata":{"id":"do805zSlwTue"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"BpwshrgrwBsn"},"source":["%tensorflow_version 2.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3pSBNlZwGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605060510446,"user_tz":-540,"elapsed":8839,"user":{"displayName":"Hiroki Nakayama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCsuu6UPzhyvNb9Oq7ud4e3-_dZ4LNQOTDJ8Md=s64","userId":"12329911095310806541"}},"outputId":"f61dd16f-c08b-4994-f0a9-6ae442d56e4f"},"source":["!pip install seqeval transformers==3.1.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n","\u001b[?25hCollecting transformers==3.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n","\u001b[K     |████████████████████████████████| 890kB 10.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 31.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 48.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2020.6.20)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=887935a318a150f17a1613af0f3c374c7d8aba8705479adaebeaef15417da10d\n","  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=0600f4e840334c007cbfeb0e49572879238daf29210d0577efd401ca5ed3f0aa\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, sentencepiece, sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 seqeval-1.2.2 tokenizers-0.8.1rc2 transformers-3.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AY7mEyiqwXw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605060511379,"user_tz":-540,"elapsed":8900,"user":{"displayName":"Hiroki Nakayama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCsuu6UPzhyvNb9Oq7ud4e3-_dZ4LNQOTDJ8Md=s64","userId":"12329911095310806541"}},"outputId":"86249741-33c6-42b1-bbd7-bee43d428270"},"source":["!mkdir data\n","!mkdir models\n","!wget https://raw.githubusercontent.com/Hironsan/IOB2Corpus/master/ja.wikipedia.conll -P data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-11 02:08:30--  https://raw.githubusercontent.com/Hironsan/IOB2Corpus/master/ja.wikipedia.conll\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1297592 (1.2M) [text/plain]\n","Saving to: ‘data/ja.wikipedia.conll’\n","\n","ja.wikipedia.conll  100%[===================>]   1.24M  --.-KB/s    in 0.06s   \n","\n","2020-11-11 02:08:30 (21.9 MB/s) - ‘data/ja.wikipedia.conll’ saved [1297592/1297592]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ENlcEzF6EyJC"},"source":["### Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"mdxgZBQKE0ac"},"source":["batch_size = 32\n","epochs = 100\n","num_words = 15000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8NdO0U1D7Zr"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"Z8cds7mUD9RL"},"source":["import re\n","\n","import numpy as np\n","import tensorflow as tf\n","from seqeval.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, Bidirectional\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7GSLaHewmV2"},"source":["## The dataset"]},{"cell_type":"markdown","metadata":{"id":"kcW-2LArE_hT"},"source":["### Load the [ja.wikipedia.conll](https://github.com/Hironsan/IOB2Corpus)"]},{"cell_type":"code","metadata":{"id":"MVaLQ4f4wkNh"},"source":["def load_dataset(filename, encoding='utf-8'):\n","    \"\"\"Loads data and label from a file.\n","    Args:\n","        filename (str): path to the file.\n","        encoding (str): file encoding format.\n","        The file format is tab-separated values.\n","        A blank line is required at the end of a sentence.\n","        For example:\n","        ```\n","        EU\tB-ORG\n","        rejects\tO\n","        German\tB-MISC\n","        call\tO\n","        to\tO\n","        boycott\tO\n","        British\tB-MISC\n","        lamb\tO\n","        .\tO\n","        Peter\tB-PER\n","        Blackburn\tI-PER\n","        ...\n","        ```\n","    Returns:\n","        tuple(numpy array, numpy array): data and labels.\n","    Example:\n","        >>> filename = 'conll2003/en/ner/train.txt'\n","        >>> data, labels = load_data_and_labels(filename)\n","    \"\"\"\n","    sents, labels = [], []\n","    words, tags = [], []\n","    with open(filename, encoding=encoding) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line:\n","                word, tag = line.split('\\t')\n","                words.append(word)\n","                tags.append(tag)\n","            else:\n","                sents.append(words)\n","                labels.append(tags)\n","                words, tags = [], []\n","        if words:\n","            sents.append(words)\n","            labels.append(tags)\n","\n","    return sents, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBjjGh7XFVzW"},"source":["x, y = load_dataset('./data/ja.wikipedia.conll')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIl_MoqCwqAu"},"source":["### Preprocess the dataset"]},{"cell_type":"code","metadata":{"id":"wBb1bBB4wpUR"},"source":["class Vocab:\n","\n","    def __init__(self, num_words=None, lower=True, oov_token=None):\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","            num_words=num_words,\n","            oov_token=oov_token,\n","            filters='',\n","            lower=lower,\n","            split='\\t'\n","        )\n","\n","    def fit(self, sequences):\n","        texts = self._texts(sequences)\n","        self.tokenizer.fit_on_texts(texts)\n","        return self\n","\n","    def encode(self, sequences):\n","        texts = self._texts(sequences)\n","        return self.tokenizer.texts_to_sequences(texts)\n","\n","    def decode(self, sequences):\n","        texts = self.tokenizer.sequences_to_texts(sequences)\n","        return [text.split(' ') for text in texts]\n","\n","    def _texts(self, sequences):\n","        return ['\\t'.join(words) for words in sequences]\n","\n","    def get_index(self, word):\n","        return self.tokenizer.word_index.get(word)\n","\n","    @property\n","    def size(self):\n","        \"\"\"Return vocabulary size.\"\"\"\n","        return len(self.tokenizer.word_index) + 1\n","\n","    def save(self, file_path):\n","        with open(file_path, 'w') as f:\n","            config = self.tokenizer.to_json()\n","            f.write(config)\n","\n","    @classmethod\n","    def load(cls, file_path):\n","        with open(file_path) as f:\n","            tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n","            vocab = cls()\n","            vocab.tokenizer = tokenizer\n","        return vocab\n","\n","\n","def normalize_number(text, reduce=True):\n","    if reduce:\n","        normalized_text = re.sub(r'\\d+', '0', text)\n","    else:\n","        normalized_text = re.sub(r'\\d', '0', text)\n","    return normalized_text\n","\n","\n","def preprocess_dataset(sequences):\n","    sequences = [[normalize_number(w) for w in words] for words in sequences]\n","    return sequences\n","\n","\n","def create_dataset(sequences, vocab):\n","    sequences = vocab.encode(sequences)\n","    sequences = pad_sequences(sequences, padding='post')\n","    return sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPz6CyN4FjCo"},"source":["x = preprocess_dataset(x)\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","source_vocab = Vocab(num_words=num_words, oov_token='<UNK>').fit(x_train)\n","target_vocab = Vocab(lower=False).fit(y_train)\n","x_train = create_dataset(x_train, source_vocab)\n","y_train = create_dataset(y_train, target_vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDhcqeEKwwDg"},"source":["## The models"]},{"cell_type":"markdown","metadata":{"id":"hdxL6fhIFth-"},"source":["### Build the models"]},{"cell_type":"code","metadata":{"id":"x2BAMgPuwuBU"},"source":["class UnidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        self.lstm = LSTM(hid_dim,\n","                         return_sequences=True,\n","                         name='lstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        lstm = self.lstm(embedding)\n","        y = self.fc(lstm)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class BidirectionalModel:\n","\n","    def __init__(self, input_dim, output_dim, emb_dim=100, hid_dim=100, embeddings=None):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       weights=[embeddings],\n","                                       name='embedding')\n","        lstm = LSTM(hid_dim,\n","                    return_sequences=True,\n","                    name='lstm')\n","        self.bilstm = Bidirectional(lstm, name='bilstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        lstm = self.bilstm(embedding)\n","        y = self.fc(lstm)\n","        return Model(inputs=x, outputs=y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGbSGjw-HHCB"},"source":["models = [\n","    UnidirectionalModel(num_words, target_vocab.size).build(),\n","    BidirectionalModel(num_words, target_vocab.size).build(),\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GR64HT-ow1A8"},"source":["### Train the models"]},{"cell_type":"code","metadata":{"id":"WaAg99slHQCj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605062034545,"user_tz":-540,"elapsed":561945,"user":{"displayName":"Hiroki Nakayama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCsuu6UPzhyvNb9Oq7ud4e3-_dZ4LNQOTDJ8Md=s64","userId":"12329911095310806541"}},"outputId":"f6a4701f-6ac4-4b1b-bf6c-869217fbc68b"},"source":["model_path = 'models/model_{}'\n","for i, model in enumerate(models):\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","\n","    # Preparing callbacks.\n","    callbacks = [\n","        EarlyStopping(patience=3),\n","        ModelCheckpoint(model_path.format(i), save_best_only=True)\n","    ]\n","\n","    # Train the model.\n","    model.fit(x=x_train,\n","              y=y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_split=0.1,\n","              callbacks=callbacks,\n","              shuffle=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","23/23 [==============================] - ETA: 0s - loss: 1.4110WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 411ms/step - loss: 1.4110 - val_loss: 0.7319\n","Epoch 2/100\n","23/23 [==============================] - ETA: 0s - loss: 0.6191INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 358ms/step - loss: 0.6191 - val_loss: 0.6258\n","Epoch 3/100\n","23/23 [==============================] - ETA: 0s - loss: 0.5763INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 370ms/step - loss: 0.5763 - val_loss: 0.6043\n","Epoch 4/100\n","23/23 [==============================] - ETA: 0s - loss: 0.5499INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 369ms/step - loss: 0.5499 - val_loss: 0.5724\n","Epoch 5/100\n","23/23 [==============================] - ETA: 0s - loss: 0.5083INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 372ms/step - loss: 0.5083 - val_loss: 0.5207\n","Epoch 6/100\n","23/23 [==============================] - ETA: 0s - loss: 0.4457INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 373ms/step - loss: 0.4457 - val_loss: 0.4635\n","Epoch 7/100\n","23/23 [==============================] - ETA: 0s - loss: 0.3864INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 361ms/step - loss: 0.3864 - val_loss: 0.4239\n","Epoch 8/100\n","23/23 [==============================] - ETA: 0s - loss: 0.3459INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 363ms/step - loss: 0.3459 - val_loss: 0.3983\n","Epoch 9/100\n","23/23 [==============================] - ETA: 0s - loss: 0.3181INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 358ms/step - loss: 0.3181 - val_loss: 0.3798\n","Epoch 10/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2963INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 373ms/step - loss: 0.2963 - val_loss: 0.3646\n","Epoch 11/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2781INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 363ms/step - loss: 0.2781 - val_loss: 0.3538\n","Epoch 12/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2617INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 374ms/step - loss: 0.2617 - val_loss: 0.3424\n","Epoch 13/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2467INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 363ms/step - loss: 0.2467 - val_loss: 0.3313\n","Epoch 14/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2343INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 370ms/step - loss: 0.2343 - val_loss: 0.3260\n","Epoch 15/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2223INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 367ms/step - loss: 0.2223 - val_loss: 0.3216\n","Epoch 16/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2114INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 367ms/step - loss: 0.2114 - val_loss: 0.3175\n","Epoch 17/100\n","23/23 [==============================] - 1s 33ms/step - loss: 0.2014 - val_loss: 0.3175\n","Epoch 18/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1922INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 358ms/step - loss: 0.1922 - val_loss: 0.3084\n","Epoch 19/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1827INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 368ms/step - loss: 0.1827 - val_loss: 0.3069\n","Epoch 20/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1730INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 9s 374ms/step - loss: 0.1730 - val_loss: 0.2980\n","Epoch 21/100\n","23/23 [==============================] - 1s 34ms/step - loss: 0.1638 - val_loss: 0.3005\n","Epoch 22/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1550INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 364ms/step - loss: 0.1550 - val_loss: 0.2935\n","Epoch 23/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1459INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 367ms/step - loss: 0.1459 - val_loss: 0.2911\n","Epoch 24/100\n","23/23 [==============================] - 1s 33ms/step - loss: 0.1374 - val_loss: 0.2923\n","Epoch 25/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1292INFO:tensorflow:Assets written to: models/model_0/assets\n","23/23 [==============================] - 8s 361ms/step - loss: 0.1292 - val_loss: 0.2881\n","Epoch 26/100\n","23/23 [==============================] - 1s 32ms/step - loss: 0.1221 - val_loss: 0.2992\n","Epoch 27/100\n","23/23 [==============================] - 1s 33ms/step - loss: 0.1146 - val_loss: 0.2883\n","Epoch 28/100\n","23/23 [==============================] - 1s 32ms/step - loss: 0.1076 - val_loss: 0.2907\n","Epoch 1/100\n","22/23 [===========================>..] - ETA: 0s - loss: 1.3537INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 20s 876ms/step - loss: 1.3391 - val_loss: 0.6642\n","Epoch 2/100\n","23/23 [==============================] - ETA: 0s - loss: 0.5942INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 816ms/step - loss: 0.5942 - val_loss: 0.6133\n","Epoch 3/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.5574INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 841ms/step - loss: 0.5568 - val_loss: 0.5813\n","Epoch 4/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.5142INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 845ms/step - loss: 0.5133 - val_loss: 0.5217\n","Epoch 5/100\n","23/23 [==============================] - ETA: 0s - loss: 0.4417INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 837ms/step - loss: 0.4417 - val_loss: 0.4536\n","Epoch 6/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.3748INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 835ms/step - loss: 0.3747 - val_loss: 0.4066\n","Epoch 7/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.3275INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 829ms/step - loss: 0.3300 - val_loss: 0.3809\n","Epoch 8/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.2979INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 841ms/step - loss: 0.2992 - val_loss: 0.3615\n","Epoch 9/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2715INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 836ms/step - loss: 0.2715 - val_loss: 0.3414\n","Epoch 10/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2460INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 830ms/step - loss: 0.2460 - val_loss: 0.3317\n","Epoch 11/100\n","23/23 [==============================] - ETA: 0s - loss: 0.2241INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 847ms/step - loss: 0.2241 - val_loss: 0.3107\n","Epoch 12/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.2030INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 843ms/step - loss: 0.2034 - val_loss: 0.3021\n","Epoch 13/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1833INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 837ms/step - loss: 0.1833 - val_loss: 0.2971\n","Epoch 14/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1637INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 845ms/step - loss: 0.1637 - val_loss: 0.2944\n","Epoch 15/100\n","23/23 [==============================] - ETA: 0s - loss: 0.1426INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 846ms/step - loss: 0.1426 - val_loss: 0.2849\n","Epoch 16/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.1242INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 19s 844ms/step - loss: 0.1235 - val_loss: 0.2801\n","Epoch 17/100\n","22/23 [===========================>..] - ETA: 0s - loss: 0.1069INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 20s 852ms/step - loss: 0.1068 - val_loss: 0.2744\n","Epoch 18/100\n","23/23 [==============================] - 1s 44ms/step - loss: 0.0922 - val_loss: 0.2804\n","Epoch 19/100\n","23/23 [==============================] - ETA: 0s - loss: 0.0805INFO:tensorflow:Assets written to: models/model_1/assets\n","23/23 [==============================] - 20s 852ms/step - loss: 0.0805 - val_loss: 0.2735\n","Epoch 20/100\n","23/23 [==============================] - 1s 45ms/step - loss: 0.0692 - val_loss: 0.2764\n","Epoch 21/100\n","23/23 [==============================] - 1s 40ms/step - loss: 0.0588 - val_loss: 0.2854\n","Epoch 22/100\n","23/23 [==============================] - 1s 40ms/step - loss: 0.0511 - val_loss: 0.2818\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BWhT3Pu5HoTB"},"source":["### Evaluate the models"]},{"cell_type":"code","metadata":{"id":"9AlqMrRUw0bK"},"source":["class InferenceAPI:\n","    \"\"\"A model API that generates output sequence.\n","\n","    Attributes:\n","        model: Model.\n","        source_vocab: source language's vocabulary.\n","        target_vocab: target language's vocabulary.\n","    \"\"\"\n","\n","    def __init__(self, model, source_vocab, target_vocab):\n","        self.model = model\n","        self.source_vocab = source_vocab\n","        self.target_vocab = target_vocab\n","\n","    def predict_from_sequences(self, sequences):\n","        lengths = map(len, sequences)\n","        sequences = self.source_vocab.encode(sequences)\n","        sequences = pad_sequences(sequences, padding='post')\n","        y_pred = self.model.predict(sequences)\n","        y_pred = np.argmax(y_pred, axis=-1)\n","        y_pred = self.target_vocab.decode(y_pred)\n","        y_pred = [y[:l] for y, l in zip(y_pred, lengths)]\n","        return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYBNwl2THxdH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605062054204,"user_tz":-540,"elapsed":19629,"user":{"displayName":"Hiroki Nakayama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCsuu6UPzhyvNb9Oq7ud4e3-_dZ4LNQOTDJ8Md=s64","userId":"12329911095310806541"}},"outputId":"d67b2a05-6180-4eb6-a900-c06fb244ad2a"},"source":["model_names = ['Unidirectional Model', 'Bidirectional Model']\n","for i, model_name in enumerate(model_names):\n","    model = load_model(model_path.format(i))\n","    api = InferenceAPI(model, source_vocab, target_vocab)\n","    y_pred = api.predict_from_sequences(x_test)\n","    y_pred = api.predict_from_sequences(x_test)\n","    print(model_name)\n","    print(classification_report(y_test, y_pred, digits=4))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unidirectional Model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    ARTIFACT     0.1274    0.1299    0.1286       154\n","        DATE     0.4012    0.8444    0.5440       315\n","       EVENT     0.0000    0.0000    0.0000        64\n","    LOCATION     0.5705    0.4924    0.5286       526\n","       MONEY     0.0000    0.0000    0.0000        12\n","      NUMBER     0.0776    0.1147    0.0926       218\n","ORGANIZATION     0.1804    0.1855    0.1829       248\n","       OTHER     0.0000    0.0000    0.0000        75\n","     PERCENT     0.0000    0.0000    0.0000        52\n","      PERSON     0.1353    0.0804    0.1008       224\n","        TIME     0.0000    0.0000    0.0000         5\n","\n","   micro avg     0.3151    0.3349    0.3247      1893\n","   macro avg     0.1357    0.1679    0.1434      1893\n","weighted avg     0.2842    0.3349    0.2944      1893\n","\n","\n","Bidirectional Model\n","              precision    recall  f1-score   support\n","\n","    ARTIFACT     0.1310    0.0714    0.0924       154\n","        DATE     0.8390    0.8603    0.8495       315\n","       EVENT     0.0357    0.0156    0.0217        64\n","    LOCATION     0.6130    0.4848    0.5414       526\n","       MONEY     0.0000    0.0000    0.0000        12\n","      NUMBER     0.3882    0.5413    0.4521       218\n","ORGANIZATION     0.2609    0.1935    0.2222       248\n","       OTHER     0.0000    0.0000    0.0000        75\n","     PERCENT     0.1467    0.2115    0.1732        52\n","      PERSON     0.0857    0.0804    0.0829       224\n","        TIME     0.0000    0.0000    0.0000         5\n","\n","   micro avg     0.4389    0.3872    0.4115      1893\n","   macro avg     0.2273    0.2235    0.2214      1893\n","weighted avg     0.4148    0.3872    0.3958      1893\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yXs9ScuGJQ4D"},"source":["# BERT for Named Entity Recognition"]},{"cell_type":"markdown","metadata":{"id":"p73SZBhOJlsX"},"source":["# utils.py"]},{"cell_type":"code","metadata":{"id":"ff6Kroz9xAdr"},"source":["def evaluate(model, target_vocab, features, labels):\n","    label_ids = model.predict(features)[0]\n","    label_ids = np.argmax(label_ids, axis=-1)\n","    y_pred = [[] for _ in range(label_ids.shape[0])]\n","    y_true = [[] for _ in range(label_ids.shape[0])]\n","    for i in range(label_ids.shape[0]):\n","        for j in range(label_ids.shape[1]):\n","            if labels[i][j] == 0:\n","                continue\n","            y_pred[i].append(label_ids[i][j])\n","            y_true[i].append(labels[i][j])\n","    y_pred = target_vocab.decode(y_pred)\n","    y_true = target_vocab.decode(y_true)\n","    print(classification_report(y_true, y_pred, digits=4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKTp7bNRJ5b2"},"source":["# preprocessing.py"]},{"cell_type":"code","metadata":{"id":"yFM1V_O-JeDo"},"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","def convert_examples_to_features(x, y,\n","                                 vocab,\n","                                 max_seq_length,\n","                                 tokenizer):\n","    pad_token = 0\n","    features = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'token_type_ids': [],\n","        'label_ids': []\n","    }\n","    for words, labels in zip(x, y):\n","        tokens = [tokenizer.cls_token]\n","        label_ids = [pad_token]\n","        for word, label in zip(words, labels):\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            label_id = vocab.get_index(label)\n","            label_ids.extend([label_id] + [pad_token] * (len(word_tokens) - 1))\n","        tokens += [tokenizer.sep_token]\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        attention_mask = [1] * len(input_ids)\n","        token_type_ids = [pad_token] * max_seq_length\n","\n","        features['input_ids'].append(input_ids)\n","        features['attention_mask'].append(attention_mask)\n","        features['token_type_ids'].append(token_type_ids)\n","        features['label_ids'].append(label_ids)\n","\n","    for name in features:\n","        features[name] = pad_sequences(features[name], padding='post', maxlen=max_seq_length)\n","\n","    x = [features['input_ids'], features['attention_mask'], features['token_type_ids']]\n","    y = features['label_ids']\n","    return x, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0aWoB26OKC1Q"},"source":["# model.py"]},{"cell_type":"code","metadata":{"id":"Uummx80GKBmK"},"source":["import tensorflow as tf\n","from transformers import TFBertForTokenClassification, BertConfig\n","\n","\n","def build_model(pretrained_model_name_or_path, num_labels):\n","    config = BertConfig.from_pretrained(\n","        pretrained_model_name_or_path,\n","        num_labels=num_labels\n","    )\n","    model = TFBertForTokenClassification.from_pretrained(\n","        pretrained_model_name_or_path,\n","        config=config\n","    )\n","    model.layers[-1].activation = tf.keras.activations.softmax\n","    return model\n","\n","\n","def loss_func(num_labels):\n","    loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def loss(y_true, y_pred):\n","        input_mask = tf.not_equal(y_true, 0)\n","        logits = tf.reshape(y_pred, (-1, num_labels))\n","        active_loss = tf.reshape(input_mask, (-1,))\n","        active_logits = tf.boolean_mask(logits, active_loss)\n","        train_labels = tf.reshape(y_true, (-1,))\n","        active_labels = tf.boolean_mask(train_labels, active_loss)\n","        cross_entropy = loss_fct(active_labels, active_logits)\n","        return cross_entropy\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-eoGpMNKKnE"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"zRhPsGBMKICn","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7e70e4dbbaf048799bf4e163b78c2a36","012132981b374dc0abbdc74d1775483d","15ce6fff54644b6dae98535f561b6922","7abfa0e91f2b4e39b16bdc457b427262","1b18548c2a02442b883155f9490e5e5a","6af325623a1d4cac99669e8cf29e6f57","ff44f17e855643788d6d5c9fc66c36ac","6633b4bb9adb48acbb67eb1aa8fbb90b","4cc3b804bca34591bc55d4619158a914","7858848c3dc0435bb47ae5ecf7e749e7","e62557cea3794df29fbf304c4b04258a","f671eaaab82a433c903756671539ce11","8bb09890ae2b49598643c833f4cdae3d","1b8f32dd61d34a3f84d6f72fedf4795f","7b906dc6aba949ff82a8ed310c216210","87710c4eb3444a0aaf13ca70eece61c1","a831c6c08135497097130e4d3edbbbd1","7f91460ebceb46aa96e88841437763f5","0fbc1970fc894e8d943c5ad7a64d26e1","d2bb35a5fc294888b2b603994f52f96a","2b0c454f81c44ffe9480eddd80f16fae","dc9bb739117445b8aa1588deb1062ccb","50aeeb7f00ac4bb388fa881a8f99b82f","3f8731e1d3954a58b9748110e8789d32"]},"executionInfo":{"status":"ok","timestamp":1599122776760,"user_tz":-540,"elapsed":772861,"user":{"displayName":"Hiroki Nakayama","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCsuu6UPzhyvNb9Oq7ud4e3-_dZ4LNQOTDJ8Md=s64","userId":"12329911095310806541"}},"outputId":"d0e972af-8aed-4e77-97e2-23470120b0d2"},"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from transformers import BertJapaneseTokenizer\n","\n","\n","def main():\n","    # Set hyper-parameters.\n","    batch_size = 16\n","    epochs = 100\n","    model_path = 'models/'\n","    pretrained_model_name_or_path = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n","    maxlen = 250\n","\n","    # Data loading.\n","    x, y = load_dataset('./data/ja.wikipedia.conll')\n","    tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_name_or_path, do_word_tokenize=False)\n","\n","    # Pre-processing.\n","    x = preprocess_dataset(x)\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","    target_vocab = Vocab(lower=False).fit(y_train)\n","    features_train, labels_train = convert_examples_to_features(\n","        x_train,\n","        y_train,\n","        target_vocab,\n","        max_seq_length=maxlen,\n","        tokenizer=tokenizer\n","    )\n","    features_test, labels_test = convert_examples_to_features(\n","        x_test,\n","        y_test,\n","        target_vocab,\n","        max_seq_length=maxlen,\n","        tokenizer=tokenizer\n","    )\n","\n","    # Build model.\n","    model = build_model(pretrained_model_name_or_path, target_vocab.size)\n","    model.compile(optimizer='sgd', loss=loss_func(target_vocab.size))\n","\n","    # Preparing callbacks.\n","    callbacks = [\n","        EarlyStopping(patience=3),\n","    ]\n","\n","    # Train the model.\n","    model.fit(x=features_train,\n","              y=labels_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_split=0.1,\n","              callbacks=callbacks,\n","              shuffle=True)\n","    model.save_pretrained(model_path)\n","\n","    # Evaluation.\n","    evaluate(model, target_vocab, features_test, labels_test)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e70e4dbbaf048799bf4e163b78c2a36","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=257706.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cc3b804bca34591bc55d4619158a914","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a831c6c08135497097130e4d3edbbbd1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=545149952.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing TFBertForTokenClassification: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['dropout_37', 'classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/100\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n","45/45 [==============================] - 23s 507ms/step - loss: 0.8492 - val_loss: 0.6682\n","Epoch 2/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.5916 - val_loss: 0.5542\n","Epoch 3/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.5106 - val_loss: 0.4973\n","Epoch 4/100\n","45/45 [==============================] - 21s 475ms/step - loss: 0.4556 - val_loss: 0.4500\n","Epoch 5/100\n","45/45 [==============================] - 21s 475ms/step - loss: 0.4096 - val_loss: 0.4082\n","Epoch 6/100\n","45/45 [==============================] - 21s 475ms/step - loss: 0.3722 - val_loss: 0.3764\n","Epoch 7/100\n","45/45 [==============================] - 21s 475ms/step - loss: 0.3442 - val_loss: 0.3481\n","Epoch 8/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.3202 - val_loss: 0.3285\n","Epoch 9/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2998 - val_loss: 0.3121\n","Epoch 10/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2834 - val_loss: 0.3013\n","Epoch 11/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2686 - val_loss: 0.2906\n","Epoch 12/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2568 - val_loss: 0.2854\n","Epoch 13/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2468 - val_loss: 0.2715\n","Epoch 14/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2383 - val_loss: 0.2670\n","Epoch 15/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2289 - val_loss: 0.2604\n","Epoch 16/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2273 - val_loss: 0.2564\n","Epoch 17/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2170 - val_loss: 0.2560\n","Epoch 18/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2111 - val_loss: 0.2476\n","Epoch 19/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2058 - val_loss: 0.2473\n","Epoch 20/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.2014 - val_loss: 0.2460\n","Epoch 21/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1948 - val_loss: 0.2485\n","Epoch 22/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1905 - val_loss: 0.2370\n","Epoch 23/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1879 - val_loss: 0.2356\n","Epoch 24/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1825 - val_loss: 0.2322\n","Epoch 25/100\n","45/45 [==============================] - 21s 473ms/step - loss: 0.1790 - val_loss: 0.2309\n","Epoch 26/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1752 - val_loss: 0.2290\n","Epoch 27/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1722 - val_loss: 0.2282\n","Epoch 28/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1696 - val_loss: 0.2287\n","Epoch 29/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1664 - val_loss: 0.2333\n","Epoch 30/100\n","45/45 [==============================] - 21s 473ms/step - loss: 0.1635 - val_loss: 0.2249\n","Epoch 31/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1603 - val_loss: 0.2244\n","Epoch 32/100\n","45/45 [==============================] - 21s 474ms/step - loss: 0.1577 - val_loss: 0.2277\n","Epoch 33/100\n","45/45 [==============================] - 21s 473ms/step - loss: 0.1557 - val_loss: 0.2258\n","Epoch 34/100\n","45/45 [==============================] - 21s 473ms/step - loss: 0.1535 - val_loss: 0.2260\n","              precision    recall  f1-score   support\n","\n","       OTHER     0.4000    0.2933    0.3385        75\n","    LOCATION     0.7688    0.7776    0.7732       526\n","ORGANIZATION     0.5277    0.6532    0.5838       248\n","      PERSON     0.7951    0.8661    0.8291       224\n","        DATE     0.7798    0.8095    0.7944       315\n","       EVENT     0.3919    0.4531    0.4203        64\n","    ARTIFACT     0.3716    0.4416    0.4036       154\n","     PERCENT     0.0000    0.0000    0.0000        52\n","        TIME     0.0000    0.0000    0.0000         5\n","      NUMBER     0.4200    0.5780    0.4865       218\n","       MONEY     0.0000    0.0000    0.0000        12\n","\n","   micro avg     0.6216    0.6683    0.6441      1893\n","   macro avg     0.6143    0.6683    0.6381      1893\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-wMu2NGaLIP2"},"source":[""],"execution_count":null,"outputs":[]}]}