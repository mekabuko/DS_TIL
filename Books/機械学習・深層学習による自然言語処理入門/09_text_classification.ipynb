{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09_text_classification.ipynb","provenance":[{"file_id":"1iAE2OUx7nFZzyrasJMYGf8fxd_fO80t7","timestamp":1627915241785},{"file_id":"1OkBqBVZ7X-9eG76Dua-SVp5Fy4xwtJO0","timestamp":1579656877473}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nsnhYVPtvJ1P"},"source":["# Text Classification with Deep Learning\n","\n","## History\n","\n","### 2020/11/3\n","\n","- Explain how to fine-tune BERT for text classification with ktrain\n","- Change `weights` to `embeddings_initializer`\n","- Call `clear_session()` when creating models in a loop to avoid OOM\n","\n","**References**\n","- [Using pre-trained word embeddings | version: Last modified: 2020/05/05](https://keras.io/examples/nlp/pretrained_word_embeddings/)\n","- [tf.keras.backend.clear_session\n","](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session)"]},{"cell_type":"markdown","metadata":{"id":"wkDOfRY-iGa6"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"MLS_APoueTC8","executionInfo":{"status":"ok","timestamp":1628489724061,"user_tz":-540,"elapsed":7,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["%tensorflow_version 2.x"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBYtpv8ZeXsX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628489748443,"user_tz":-540,"elapsed":24386,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"a4b4fd6f-884c-4be9-bf13-d11267c722c0"},"source":["!pip install janome beautifulsoup4"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting janome\n","  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n","\u001b[K     |████████████████████████████████| 19.7 MB 53.5 MB/s \n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n","Installing collected packages: janome\n","Successfully installed janome-0.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nZMUsGqQec6Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628489786931,"user_tz":-540,"elapsed":38492,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"1829e0fa-9f06-456c-d87c-ec3b261fdd1d"},"source":["!mkdir data\n","!mkdir models\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz -P data/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-08-09 06:15:48--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1279641604 (1.2G) [binary/octet-stream]\n","Saving to: ‘data/cc.ja.300.vec.gz’\n","\n","cc.ja.300.vec.gz    100%[===================>]   1.19G  38.3MB/s    in 38s     \n","\n","2021-08-09 06:16:26 (32.2 MB/s) - ‘data/cc.ja.300.vec.gz’ saved [1279641604/1279641604]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WwGX88rvEO4l"},"source":["### Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"hIIwk_n3EVqO","executionInfo":{"status":"ok","timestamp":1628489786932,"user_tz":-540,"elapsed":8,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["maxlen = 300\n","num_words = 40000\n","num_label = 2"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tn2IFMP9vVCM"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"2YGTG_tXvZCe","executionInfo":{"status":"ok","timestamp":1628489800966,"user_tz":-540,"elapsed":2760,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["import string\n","\n","import gensim\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from bs4 import BeautifulSoup\n","from janome.tokenizer import Tokenizer\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Dense, Input, Embedding, SimpleRNN, LSTM, Conv1D, GlobalMaxPooling1D\n","from tensorflow.keras.models import load_model, Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ubOwS_mfh4Bu"},"source":["## The dataset"]},{"cell_type":"markdown","metadata":{"id":"8ngFa4k4wAt3"},"source":["### Load the Amazon Customer Reviews Datasets"]},{"cell_type":"code","metadata":{"id":"NldWkmWqg3eD","executionInfo":{"status":"ok","timestamp":1628489818250,"user_tz":-540,"elapsed":17289,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["def filter_by_ascii_rate(text, threshold=0.9):\n","    ascii_letters = set(string.printable)\n","    rate = sum(c in ascii_letters for c in text) / len(text)\n","    return rate <= threshold\n","\n","\n","def load_dataset(filename, n=5000):\n","    df = pd.read_csv(filename, sep='\\t')\n","\n","    # Converts multi-class to binary-class.\n","    mapping = {1: 0, 2: 0, 4: 1, 5: 1}\n","    df = df[df.star_rating != 3]\n","    df.star_rating = df.star_rating.map(mapping)\n","\n","    # extracts Japanese texts.\n","    is_jp = df.review_body.apply(filter_by_ascii_rate)\n","    df = df[is_jp]\n","\n","    # sampling.\n","    df = df.sample(frac=1, random_state=7)  # shuffle\n","    grouped = df.groupby('star_rating')\n","    df = grouped.head(n=n)\n","    return df.review_body.values, df.star_rating.values\n","\n","\n","url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz'\n","x, y = load_dataset(url)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1cLWKlqpwoeL"},"source":["### Load the word embeddings"]},{"cell_type":"code","metadata":{"id":"md9Iun1fwrTI","executionInfo":{"status":"ok","timestamp":1628490370801,"user_tz":-540,"elapsed":552565,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["def load_fasttext(filepath, binary=False):\n","    \"\"\"Loads fastText vectors.\n","\n","    Args:\n","        filepath (str): a path to a fastText file.\n","\n","    Return:\n","        model: KeyedVectors\n","    \"\"\"\n","    model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=binary)\n","    return model\n","\n","\n","wv = load_fasttext('data/cc.ja.300.vec.gz')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYdugMjzh8yz"},"source":["### Preprocess the dataset"]},{"cell_type":"code","metadata":{"id":"RWlQYG-phz0n","executionInfo":{"status":"ok","timestamp":1628490370802,"user_tz":-540,"elapsed":16,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["t = Tokenizer(wakati=True)\n","\n","\n","def build_vocabulary(texts, num_words=None):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=num_words, oov_token='<UNK>'\n","    )\n","    tokenizer.fit_on_texts(texts)\n","    return tokenizer\n","\n","\n","def clean_html(html, strip=False):\n","    soup = BeautifulSoup(html, 'html.parser')\n","    text = soup.get_text(strip=strip)\n","    return text\n","\n","\n","def tokenize(text):\n","    return t.tokenize(text)\n","\n","\n","def preprocess_dataset(texts):\n","    texts = [clean_html(text) for text in texts]\n","    texts = [' '.join(tokenize(text)) for text in texts]\n","    return texts\n","\n","\n","def filter_embeddings(embeddings, vocab, num_words, dim=300):\n","  \"\"\"Filter word vectors.\n","\n","  Args:\n","      embeddings: a dictionary like object.\n","      vocab: word-index lookup table.\n","      num_words: the number of words.\n","      dim: dimension.\n","\n","  Returns:\n","      numpy array: an array of word embeddings.\n","  \"\"\"\n","  _embeddings = np.zeros((num_words, dim))\n","  for word in vocab:\n","      if word in embeddings:\n","          word_id = vocab[word]\n","          if word_id >= num_words:\n","              continue\n","          _embeddings[word_id] = embeddings[word]\n","\n","  return _embeddings"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"XG7xi7LYFTMs","executionInfo":{"status":"ok","timestamp":1628490563669,"user_tz":-540,"elapsed":192880,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["x = preprocess_dataset(x)\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","vocab = build_vocabulary(x_train, num_words)\n","x_train = vocab.texts_to_sequences(x_train)\n","x_test = vocab.texts_to_sequences(x_test)\n","x_train = pad_sequences(x_train, maxlen=maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen=maxlen, truncating='post', padding='post')\n","\n","wv = filter_embeddings(wv, vocab.word_index, num_words)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49qdjUsFurRX"},"source":["## The models"]},{"cell_type":"markdown","metadata":{"id":"RodOCCr2wfHu"},"source":["### Build the models"]},{"cell_type":"code","metadata":{"id":"vSORC-J0usu4","executionInfo":{"status":"ok","timestamp":1628490563671,"user_tz":-540,"elapsed":10,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["class RNNModel:\n","\n","    def __init__(self, input_dim, output_dim,\n","                 emb_dim=300, hid_dim=100,\n","                 embeddings=None, trainable=True):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       trainable=trainable,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       trainable=trainable,\n","                                       embeddings_initializer=tf.keras.initializers.Constant(embeddings),\n","                                       # weights=[embeddings],\n","                                       name='embedding')\n","        self.rnn = SimpleRNN(hid_dim, name='rnn')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        output = self.rnn(embedding)\n","        y = self.fc(output)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class LSTMModel:\n","\n","    def __init__(self, input_dim, output_dim,\n","                 emb_dim=300, hid_dim=100,\n","                 embeddings=None, trainable=True):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       mask_zero=True,\n","                                       trainable=trainable,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       mask_zero=True,\n","                                       trainable=trainable,\n","                                       embeddings_initializer=tf.keras.initializers.Constant(embeddings),\n","                                       # weights=[embeddings],\n","                                       name='embedding')\n","        self.lstm = LSTM(hid_dim, name='lstm')\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        output = self.lstm(embedding)\n","        y = self.fc(output)\n","        return Model(inputs=x, outputs=y)\n","\n","\n","class CNNModel:\n","\n","    def __init__(self, input_dim, output_dim,\n","                 filters=250, kernel_size=3,\n","                 emb_dim=300, embeddings=None, trainable=True):\n","        self.input = Input(shape=(None,), name='input')\n","        if embeddings is None:\n","            self.embedding = Embedding(input_dim=input_dim,\n","                                       output_dim=emb_dim,\n","                                       trainable=trainable,\n","                                       name='embedding')\n","        else:\n","            self.embedding = Embedding(input_dim=embeddings.shape[0],\n","                                       output_dim=embeddings.shape[1],\n","                                       trainable=trainable,\n","                                       embeddings_initializer=tf.keras.initializers.Constant(embeddings),\n","                                       # weights=[embeddings],\n","                                       name='embedding')\n","        self.conv = Conv1D(filters,\n","                           kernel_size,\n","                           padding='valid',\n","                           activation='relu',\n","                           strides=1)\n","        self.pool = GlobalMaxPooling1D()\n","        self.fc = Dense(output_dim, activation='softmax')\n","\n","    def build(self):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        conv = self.conv(embedding)\n","        pool = self.pool(conv)\n","        y = self.fc(pool)\n","        return Model(inputs=x, outputs=y)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"auhlTfsbF477","executionInfo":{"status":"ok","timestamp":1628490563671,"user_tz":-540,"elapsed":6,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["models = [\n","    RNNModel,\n","    LSTMModel,\n","    CNNModel,\n","    CNNModel\n","]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_BG3pF0nGIYQ"},"source":["### Train the models"]},{"cell_type":"code","metadata":{"id":"2A62IICsGUTw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491647793,"user_tz":-540,"elapsed":546623,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"cb82292e-7dc6-4be9-b961-f305682fd895"},"source":["model_path = 'models/model_{}'\n","embeddings = [None, None, None, wv]\n","batch_size = 128\n","epochs = 100\n","i = 0\n","for model, embedding in zip(models, embeddings):\n","    tf.keras.backend.clear_session()\n","    model = model(num_words, num_label, embeddings=embedding).build()\n","    model.compile(\n","        optimizer='adam',\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['acc']\n","    )\n","\n","    callbacks = [\n","        EarlyStopping(patience=3),\n","        ModelCheckpoint(model_path.format(i), save_best_only=True)\n","    ]\n","\n","    model.fit(\n","        x=x_train, y=y_train,\n","        batch_size=batch_size,\n","        epochs=epochs,\n","        validation_split=0.2,\n","        callbacks=callbacks,\n","        shuffle=True\n","    )\n","    i += 1"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","50/50 [==============================] - 24s 407ms/step - loss: 0.6381 - acc: 0.6242 - val_loss: 0.5420 - val_acc: 0.7400\n","INFO:tensorflow:Assets written to: models/model_0/assets\n","Epoch 2/100\n","50/50 [==============================] - 20s 391ms/step - loss: 0.2350 - acc: 0.9123 - val_loss: 0.6489 - val_acc: 0.7138\n","Epoch 3/100\n","50/50 [==============================] - 21s 416ms/step - loss: 0.0427 - acc: 0.9875 - val_loss: 0.6609 - val_acc: 0.7419\n","Epoch 4/100\n","50/50 [==============================] - 20s 407ms/step - loss: 0.0112 - acc: 0.9981 - val_loss: 0.7179 - val_acc: 0.7681\n","Epoch 1/100\n","50/50 [==============================] - 15s 192ms/step - loss: 0.5624 - acc: 0.7147 - val_loss: 0.4676 - val_acc: 0.7831\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_1/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_1/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2/100\n","50/50 [==============================] - 9s 182ms/step - loss: 0.2741 - acc: 0.8936 - val_loss: 0.4322 - val_acc: 0.8131\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_1/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_1/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3/100\n","50/50 [==============================] - 9s 183ms/step - loss: 0.0992 - acc: 0.9672 - val_loss: 0.6200 - val_acc: 0.7981\n","Epoch 4/100\n","50/50 [==============================] - 9s 181ms/step - loss: 0.0463 - acc: 0.9850 - val_loss: 0.9933 - val_acc: 0.7731\n","Epoch 5/100\n","50/50 [==============================] - 9s 184ms/step - loss: 0.0233 - acc: 0.9942 - val_loss: 0.9324 - val_acc: 0.7919\n","Epoch 1/100\n","50/50 [==============================] - 34s 167ms/step - loss: 0.6220 - acc: 0.6700 - val_loss: 0.5500 - val_acc: 0.7375\n","INFO:tensorflow:Assets written to: models/model_2/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_2/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2/100\n","50/50 [==============================] - 8s 163ms/step - loss: 0.4097 - acc: 0.8386 - val_loss: 0.4301 - val_acc: 0.8044\n","INFO:tensorflow:Assets written to: models/model_2/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_2/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3/100\n","50/50 [==============================] - 8s 163ms/step - loss: 0.2194 - acc: 0.9298 - val_loss: 0.4015 - val_acc: 0.8250\n","INFO:tensorflow:Assets written to: models/model_2/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_2/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 4/100\n","50/50 [==============================] - 8s 164ms/step - loss: 0.0897 - acc: 0.9819 - val_loss: 0.4086 - val_acc: 0.8425\n","Epoch 5/100\n","50/50 [==============================] - 8s 166ms/step - loss: 0.0281 - acc: 0.9986 - val_loss: 0.4474 - val_acc: 0.8338\n","Epoch 6/100\n","50/50 [==============================] - 8s 165ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.4859 - val_acc: 0.8306\n","Epoch 1/100\n","50/50 [==============================] - 9s 171ms/step - loss: 0.5846 - acc: 0.6839 - val_loss: 0.4750 - val_acc: 0.7850\n","INFO:tensorflow:Assets written to: models/model_3/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_3/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2/100\n","50/50 [==============================] - 8s 158ms/step - loss: 0.3374 - acc: 0.8806 - val_loss: 0.3987 - val_acc: 0.8306\n","INFO:tensorflow:Assets written to: models/model_3/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_3/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3/100\n","50/50 [==============================] - 8s 160ms/step - loss: 0.2037 - acc: 0.9489 - val_loss: 0.3693 - val_acc: 0.8419\n","INFO:tensorflow:Assets written to: models/model_3/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: models/model_3/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 4/100\n","50/50 [==============================] - 8s 161ms/step - loss: 0.1093 - acc: 0.9792 - val_loss: 0.3746 - val_acc: 0.8425\n","Epoch 5/100\n","50/50 [==============================] - 8s 160ms/step - loss: 0.0533 - acc: 0.9961 - val_loss: 0.3827 - val_acc: 0.8475\n","Epoch 6/100\n","50/50 [==============================] - 8s 157ms/step - loss: 0.0261 - acc: 0.9992 - val_loss: 0.4106 - val_acc: 0.8413\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XJ40Y895G4yh"},"source":["### Evaluate the models"]},{"cell_type":"code","metadata":{"id":"eKuUr5h4uwb8","executionInfo":{"status":"ok","timestamp":1628491647793,"user_tz":-540,"elapsed":12,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["class InferenceAPI:\n","    \"\"\"A model API that generates output sequence.\n","\n","    Attributes:\n","        model: Model.\n","        vocab: language's vocabulary.\n","    \"\"\"\n","\n","    def __init__(self, model, vocab, preprocess):\n","        self.model = model\n","        self.vocab = vocab\n","        self.preprocess = preprocess\n","\n","    def predict_from_texts(self, texts):\n","        x = self.preprocess(texts)\n","        x = self.vocab.texts_to_sequences(x)\n","        return self.predict_from_sequences(x)\n","\n","    def predict_from_sequences(self, sequences):\n","        sequences = pad_sequences(sequences, truncating='post')\n","        y = self.model.predict(sequences)\n","        return np.argmax(y, -1)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4meeU3cG9Lg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491723102,"user_tz":-540,"elapsed":75311,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"4fe745ad-e9e3-4a87-9722-6bf4ff13229b"},"source":["model_names = ['RNN', 'LSTM', 'CNN', 'CNN(wv)']\n","for i, model_name in enumerate(model_names):\n","    tf.keras.backend.clear_session()\n","    model = load_model(model_path.format(i))\n","    api = InferenceAPI(model, vocab, preprocess_dataset)\n","    y_pred = api.predict_from_sequences(x_test)\n","    print(model_name)\n","    print('precision\\t: {:.4f}'.format(precision_score(y_test, y_pred, average='binary')))\n","    print('recall\\t: {:.4f}'.format(recall_score(y_test, y_pred, average='binary')))\n","    print('f1\\t: {:.4f}'.format(f1_score(y_test, y_pred, average='binary')))\n","    print()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["RNN\n","precision\t: 0.8055\n","recall\t: 0.6513\n","f1\t: 0.7202\n","\n","LSTM\n","precision\t: 0.8037\n","recall\t: 0.8287\n","f1\t: 0.8160\n","\n","CNN\n","precision\t: 0.7989\n","recall\t: 0.8958\n","f1\t: 0.8446\n","\n","CNN(wv)\n","precision\t: 0.8587\n","recall\t: 0.8096\n","f1\t: 0.8334\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_jS-zFeYOi0k"},"source":["# Fine-tune BERT for Text Classification with ktrain\n","\n","文量の都合上、書籍には含まれていませんが、東北大が公開している日本語のBERTをFine-tuneして文章分類をしてみましょう。現在では、BERTを使う場合、Huggingfaceの[Transformers](https://github.com/huggingface/transformers)というパッケージが使われることが多いですが、ここではよりシンプルに書ける[ktrain](https://github.com/amaiya/ktrain)を使ってみます。細かい設定はともかく、サクッと学習させたいときに便利です。"]},{"cell_type":"markdown","metadata":{"id":"Pg904nB9Otks"},"source":["## Setup\n","\n","MeCabとktrainをインストールします。MeCabはBERTのTokenizerの中で使われています。"]},{"cell_type":"code","metadata":{"id":"WKpl8Gh8Opeh","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1628491795182,"user_tz":-540,"elapsed":72094,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"70d5aa0a-3467-4782-afbc-37a8982c8a87"},"source":["!apt install aptitude\n","!aptitude install mecab libmecab-dev -y\n","!pip install mecab-python3 fugashi ipadic\n","!pip install ktrain"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n","  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n","  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n","  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n","  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n","Suggested packages:\n","  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n","  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n","  libwww-perl xapian-tools\n","The following NEW packages will be installed:\n","  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n","  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n","  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n","  libhttp-message-perl libio-html-perl libio-string-perl\n","  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n","  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n","0 upgraded, 21 newly installed, 0 to remove and 40 not upgraded.\n","Need to get 3,877 kB of archives.\n","After this operation, 15.6 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n","Fetched 3,877 kB in 2s (2,195 kB/s)\n","Selecting previously unselected package aptitude-common.\n","(Reading database ... 160837 files and directories currently installed.)\n","Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n","Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n","Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n","Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Selecting previously unselected package libcwidget3v5:amd64.\n","Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n","Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n","Selecting previously unselected package libxapian30:amd64.\n","Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n","Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Selecting previously unselected package aptitude.\n","Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n","Unpacking aptitude (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libhtml-tagset-perl.\n","Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n","Unpacking libhtml-tagset-perl (3.20-3) ...\n","Selecting previously unselected package liburi-perl.\n","Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n","Unpacking liburi-perl (1.73-1) ...\n","Selecting previously unselected package libhtml-parser-perl.\n","Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n","Unpacking libhtml-parser-perl (3.72-3build1) ...\n","Selecting previously unselected package libcgi-pm-perl.\n","Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n","Unpacking libcgi-pm-perl (4.38-1) ...\n","Selecting previously unselected package libfcgi-perl.\n","Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n","Unpacking libfcgi-perl (0.78-2build1) ...\n","Selecting previously unselected package libcgi-fast-perl.\n","Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n","Unpacking libcgi-fast-perl (1:2.13-1) ...\n","Selecting previously unselected package libsub-name-perl.\n","Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n","Unpacking libsub-name-perl (0.21-1build1) ...\n","Selecting previously unselected package libclass-accessor-perl.\n","Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n","Unpacking libclass-accessor-perl (0.51-1) ...\n","Selecting previously unselected package libencode-locale-perl.\n","Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n","Unpacking libencode-locale-perl (1.05-1) ...\n","Selecting previously unselected package libtimedate-perl.\n","Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n","Unpacking libtimedate-perl (2.3000-2) ...\n","Selecting previously unselected package libhttp-date-perl.\n","Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n","Unpacking libhttp-date-perl (6.02-1) ...\n","Selecting previously unselected package libio-html-perl.\n","Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n","Unpacking libio-html-perl (1.001-1) ...\n","Selecting previously unselected package liblwp-mediatypes-perl.\n","Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n","Unpacking liblwp-mediatypes-perl (6.02-1) ...\n","Selecting previously unselected package libhttp-message-perl.\n","Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n","Unpacking libhttp-message-perl (6.14-1) ...\n","Selecting previously unselected package libio-string-perl.\n","Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n","Unpacking libio-string-perl (1.08-3) ...\n","Selecting previously unselected package libparse-debianchangelog-perl.\n","Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n","Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n","Setting up libhtml-tagset-perl (3.20-3) ...\n","Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Setting up libencode-locale-perl (1.05-1) ...\n","Setting up libtimedate-perl (2.3000-2) ...\n","Setting up libio-html-perl (1.001-1) ...\n","Setting up aptitude-common (0.8.10-6ubuntu1) ...\n","Setting up liblwp-mediatypes-perl (6.02-1) ...\n","Setting up liburi-perl (1.73-1) ...\n","Setting up libhtml-parser-perl (3.72-3build1) ...\n","Setting up libcgi-pm-perl (4.38-1) ...\n","Setting up libio-string-perl (1.08-3) ...\n","Setting up libsub-name-perl (0.21-1build1) ...\n","Setting up libfcgi-perl (0.78-2build1) ...\n","Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Setting up libclass-accessor-perl (0.51-1) ...\n","Setting up libhttp-date-perl (6.02-1) ...\n","Setting up libcgi-fast-perl (1:2.13-1) ...\n","Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n","Setting up libhttp-message-perl (6.14-1) ...\n","Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n","Setting up aptitude (0.8.10-6ubuntu1) ...\n","update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","The following NEW packages will be installed:\n","  libmecab-dev libmecab2{a} mecab mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n","0 packages upgraded, 6 newly installed, 0 to remove and 40 not upgraded.\n","Need to get 16.9 MB of archives. After unpacking 222 MB will be used.\n","Get: 1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n","Get: 2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n","Get: 3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n","Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n","Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n","Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n","Fetched 16.9 MB in 2s (7,319 kB/s)\n","Selecting previously unselected package libmecab2:amd64.\n","(Reading database ... 161296 files and directories currently installed.)\n","Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n","Unpacking libmecab2:amd64 (0.996-5) ...\n","Selecting previously unselected package libmecab-dev.\n","Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n","Unpacking libmecab-dev (0.996-5) ...\n","Selecting previously unselected package mecab-utils.\n","Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n","Unpacking mecab-utils (0.996-5) ...\n","Selecting previously unselected package mecab-jumandic-utf8.\n","Preparing to unpack .../3-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-jumandic.\n","Preparing to unpack .../4-mecab-jumandic_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic (7.0-20130310-4) ...\n","Selecting previously unselected package mecab.\n","Preparing to unpack .../5-mecab_0.996-5_amd64.deb ...\n","Unpacking mecab (0.996-5) ...\n","Setting up libmecab2:amd64 (0.996-5) ...\n","Setting up mecab-utils (0.996-5) ...\n","Setting up libmecab-dev (0.996-5) ...\n","Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Compiling Juman dictionary for Mecab.\n","reading /usr/share/mecab/dic/juman/unk.def ... 37\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n","reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n","reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n","reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n","reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n","reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n","reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n","reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n","reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n","reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n","reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n","reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n","reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n","reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n","reading /usr/share/mecab/dic/juman/Special.csv ... 158\n","reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/juman-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up mecab (0.996-5) ...\n","Setting up mecab-jumandic (7.0-20130310-4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","                            \n","Collecting mecab-python3\n","  Downloading mecab_python3-1.0.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (488 kB)\n","\u001b[K     |████████████████████████████████| 488 kB 8.8 MB/s \n","\u001b[?25hCollecting fugashi\n","  Downloading fugashi-1.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (490 kB)\n","\u001b[K     |████████████████████████████████| 490 kB 43.1 MB/s \n","\u001b[?25hCollecting ipadic\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[K     |████████████████████████████████| 13.4 MB 220 kB/s \n","\u001b[?25hBuilding wheels for collected packages: ipadic\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=41642709b28c8d0ef3867a2fe41f56df02644e16856ec1fbabb29e0ce26828b6\n","  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n","Successfully built ipadic\n","Installing collected packages: mecab-python3, ipadic, fugashi\n","Successfully installed fugashi-1.1.1 ipadic-1.0.0 mecab-python3-1.0.4\n","Collecting ktrain\n","  Downloading ktrain-0.27.2.tar.gz (25.3 MB)\n","\u001b[K     |████████████████████████████████| 25.3 MB 84 kB/s \n","\u001b[?25hCollecting scikit-learn==0.23.2\n","  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 35.9 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.5)\n","Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ktrain) (5.5.0)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 48.6 MB/s \n","\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n","Collecting cchardet\n","  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n","\u001b[K     |████████████████████████████████| 263 kB 67.9 MB/s \n","\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n","Collecting syntok\n","  Downloading syntok-1.3.1.tar.gz (23 kB)\n","Collecting seqeval==0.0.19\n","  Downloading seqeval-0.0.19.tar.gz (30 kB)\n","Collecting transformers<=4.3.3,>=4.0.0\n","  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 21.7 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 46.1 MB/s \n","\u001b[?25hCollecting keras_bert>=0.86.0\n","  Downloading keras-bert-0.88.0.tar.gz (26 kB)\n","Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.5.1)\n","Collecting whoosh\n","  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n","\u001b[K     |████████████████████████████████| 468 kB 60.1 MB/s \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.19.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.19->ktrain) (2.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (3.1.0)\n","Collecting keras-transformer>=0.39.0\n","  Downloading keras-transformer-0.39.0.tar.gz (11 kB)\n","Collecting keras-pos-embd>=0.12.0\n","  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n","Collecting keras-multi-head>=0.28.0\n","  Downloading keras-multi-head-0.28.0.tar.gz (14 kB)\n","Collecting keras-layer-normalization>=0.15.0\n","  Downloading keras-layer-normalization-0.15.0.tar.gz (4.2 kB)\n","Collecting keras-position-wise-feed-forward>=0.7.0\n","  Downloading keras-position-wise-feed-forward-0.7.0.tar.gz (4.5 kB)\n","Collecting keras-embed-sim>=0.9.0\n","  Downloading keras-embed-sim-0.9.0.tar.gz (4.1 kB)\n","Collecting keras-self-attention>=0.50.0\n","  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->ktrain) (1.15.0)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.41.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 50.9 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 56.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.6.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.2.4->seqeval==0.0.19->ktrain) (1.5.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.3.3,>=4.0.0->ktrain) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.3.3,>=4.0.0->ktrain) (3.5.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (5.0.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (57.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (2.6.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ktrain) (0.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.3.3,>=4.0.0->ktrain) (7.1.2)\n","Building wheels for collected packages: ktrain, seqeval, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, syntok\n","  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ktrain: filename=ktrain-0.27.2-py3-none-any.whl size=25283087 sha256=3bb2903d8a4c4b874bbb7b4d5100713fb1dbcb62acff659fb8519aa4a8e8caa8\n","  Stored in directory: /root/.cache/pip/wheels/88/be/4a/971c83a380a40f12e877f643ca1b94dc65f528f94c88dbcff7\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.19-py3-none-any.whl size=9929 sha256=eb0ec9f8dd4418d2d89842582d5c28401e339661faa4ea918d65f77a8ab48d00\n","  Stored in directory: /root/.cache/pip/wheels/f5/ac/f1/4e13d7aff05c722d142b7d20a88ad63f9aab11b895411241a4\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.88.0-py3-none-any.whl size=34205 sha256=87079fdea5fd866547b1f0d10be6a0c5250ebade1c00f68da44e6a5ca0692686\n","  Stored in directory: /root/.cache/pip/wheels/a2/90/cd/c038f2366929a3a5e3414a303b673e10235e802d871d29a835\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-py3-none-any.whl size=12840 sha256=9b0466d4222185700c71e1032f7cd5358c58e3e3f04b701ba4637522a3a555b8\n","  Stored in directory: /root/.cache/pip/wheels/bc/01/e0/5a1a14bed6726f2ed73f7917d2d2c2d4081d2c88426dea07ce\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-py3-none-any.whl size=4506 sha256=8d172b891bf5613382298a61edebb9bbd34092fd101edf1e114da017e3a7feef\n","  Stored in directory: /root/.cache/pip/wheels/a8/1e/d2/9bc15513dd2f8b9de3e628b3aa9d2de49e721deef6bbd1497e\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-py3-none-any.whl size=5224 sha256=4e488dd6388d5e03a408d265ebbceca64b0de052dca4f4d7d07f7c366f6aef40\n","  Stored in directory: /root/.cache/pip/wheels/4d/be/fe/55422f77ac11fe6ddcb471198038de8a26b5a4dd1557883c1e\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-py3-none-any.whl size=15560 sha256=d07b81317a536d9c2d75826e2d1eed89e6d4883e448c031e0937880ab4ec115c\n","  Stored in directory: /root/.cache/pip/wheels/79/4a/ea/9503ab5a02201dfb8635ba2cc8f30844661623c684a5b44472\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7470 sha256=8794600593a4aa6ba7b615db5227558b82225c59f57bfe390e50302f771060fd\n","  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-py3-none-any.whl size=5541 sha256=afd0a6407217373356e995b711da3c6fbb1b68741dd8be6a897829a020efa396\n","  Stored in directory: /root/.cache/pip/wheels/2d/12/02/1ad455c4f181cda1a4e60c5445855853d5c2ea91f942586a04\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19415 sha256=cbcbfbed7011ded2a646a237cbae81e3c126458f483f6ac7303ccf9377af0c64\n","  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=f0c1d5810f41d712a260019d48e14557b986408287dc8826230ec8d573e4a961\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20917 sha256=cd1ec12c420d2661513740b0b41e98f94b80f2fab2a98352a65016a1bb310c35\n","  Stored in directory: /root/.cache/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\n","Successfully built ktrain seqeval keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect syntok\n","Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, threadpoolctl, sacremoses, keras-transformer, whoosh, transformers, syntok, seqeval, sentencepiece, scikit-learn, langdetect, keras-bert, cchardet, ktrain\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed cchardet-2.1.7 keras-bert-0.88.0 keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0 ktrain-0.27.2 langdetect-1.0.9 sacremoses-0.0.45 scikit-learn-0.23.2 sentencepiece-0.1.96 seqeval-0.0.19 syntok-1.3.1 threadpoolctl-2.2.0 tokenizers-0.10.3 transformers-4.3.3 whoosh-2.7.4\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["sklearn"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"rHqwZJppPkjV"},"source":["### Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"502K8VkHPml9","executionInfo":{"status":"ok","timestamp":1628491795183,"user_tz":-540,"elapsed":5,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["maxlen = 300\n","lr = 2e-5\n","epochs = 2\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xEicJMpxPEap"},"source":["### Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBz_t7I_syZM","executionInfo":{"status":"ok","timestamp":1628492216505,"user_tz":-540,"elapsed":3296,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"3fc93316-db6a-489b-ded1-05de5989592b"},"source":["! pip install ktrain"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ktrain in /usr/local/lib/python3.7/dist-packages (0.27.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.5)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n","Requirement already satisfied: whoosh in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.7.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.1.96)\n","Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.0)\n","Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.0)\n","Requirement already satisfied: syntok in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.1)\n","Requirement already satisfied: seqeval==0.0.19 in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.0.19)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ktrain) (5.5.0)\n","Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.23.2)\n","Requirement already satisfied: keras-bert>=0.86.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.88.0)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.9)\n","Requirement already satisfied: transformers<=4.3.3,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (4.3.3)\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.1)\n","Requirement already satisfied: cchardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.1.7)\n","Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.19.5)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (2.2.0)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.19->ktrain) (2.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (3.1.0)\n","Requirement already satisfied: keras-transformer>=0.39.0 in /usr/local/lib/python3.7/dist-packages (from keras-bert>=0.86.0->ktrain) (0.39.0)\n","Requirement already satisfied: keras-position-wise-feed-forward>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.39.0->keras-bert>=0.86.0->ktrain) (0.7.0)\n","Requirement already satisfied: keras-pos-embd>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.39.0->keras-bert>=0.86.0->ktrain) (0.12.0)\n","Requirement already satisfied: keras-embed-sim>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.39.0->keras-bert>=0.86.0->ktrain) (0.9.0)\n","Requirement already satisfied: keras-layer-normalization>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.39.0->keras-bert>=0.86.0->ktrain) (0.15.0)\n","Requirement already satisfied: keras-multi-head>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.39.0->keras-bert>=0.86.0->ktrain) (0.28.0)\n","Requirement already satisfied: keras-self-attention>=0.50.0 in /usr/local/lib/python3.7/dist-packages (from keras-multi-head>=0.28.0->keras-transformer>=0.39.0->keras-bert>=0.86.0->ktrain) (0.50.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->ktrain) (1.15.0)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (0.10.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (0.0.45)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.41.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.6.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.2.4->seqeval==0.0.19->ktrain) (1.5.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.3.3,>=4.0.0->ktrain) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.3.3,>=4.0.0->ktrain) (3.5.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (5.0.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (2.6.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (57.2.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ktrain) (0.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.3.3,>=4.0.0->ktrain) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxoQyajNOrr_","colab":{"base_uri":"https://localhost:8080/","height":464},"executionInfo":{"status":"error","timestamp":1628492292328,"user_tz":-540,"elapsed":268,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}},"outputId":"d849fd99-d3fd-459e-ab72-74196c23768f"},"source":["from ktrain import text\n","from sklearn.metrics import classification_report"],"execution_count":24,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-34fd81519a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mktrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimports\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelease_gpu_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTTextClassLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerTextClassLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/imports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_covtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_covtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kddcup99\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_kddcup99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lfw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_lfw_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lfw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_lfw_people\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_twenty_newsgroups\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_lfw.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'parse_version' from 'sklearn.utils.fixes' (/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"B4WT1GGbPZuM"},"source":["## The dataset"]},{"cell_type":"markdown","metadata":{"id":"nA2nJi8UPb12"},"source":["### Preprocess the dataset\n","\n","前処理としては、HTMLタグの除去とテキストをBERTに入力できる形式に変換しています。`text.Transformer`を使うことで、HuggingfaceのTransformersをラップして利用することができます。"]},{"cell_type":"code","metadata":{"id":"AqBzZVJdPavh","executionInfo":{"status":"aborted","timestamp":1628491796050,"user_tz":-540,"elapsed":485,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["x = [clean_html(text) for text in x]\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBgVhD4HPwLl","executionInfo":{"status":"aborted","timestamp":1628491796051,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["classes = list(set(y_train))\n","t = text.Transformer(MODEL_NAME, maxlen=maxlen, class_names=classes)\n","trn = t.preprocess_train(x_train, y_train)\n","val = t.preprocess_train(x_val, y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t25BZH57P0zv"},"source":["## The models"]},{"cell_type":"markdown","metadata":{"id":"Oo2utsr7P3mc"},"source":["### Build the model"]},{"cell_type":"code","metadata":{"id":"YAtpT3ecP2T9","executionInfo":{"status":"aborted","timestamp":1628491796051,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["model = t.get_classifier()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AShqIcBP7Iu"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"ZkTE27OpP8Kt","executionInfo":{"status":"aborted","timestamp":1628491796051,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=8)\n","learner.fit_onecycle(lr, epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15ndSHZCQMyo"},"source":["### Evaluate the model"]},{"cell_type":"code","metadata":{"id":"SfBp_H2sQN9K","executionInfo":{"status":"aborted","timestamp":1628491796051,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["predictor = ktrain.get_predictor(learner.model, preproc=t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nUFdfk2QSX-","executionInfo":{"status":"aborted","timestamp":1628491796051,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["y_pred = predictor.predict(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5RfGwtykQVcf","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["print(classification_report(y_test, y_pred, digits=4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VS1yhTMiQmiX"},"source":["### Save and Load the model"]},{"cell_type":"code","metadata":{"id":"VJEAiPa2Qop2","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["predictor.save('/tmp/model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CkONIzxQs7C","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["reloaded_predictor = ktrain.load_predictor('/tmp/model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hx9uZY4cehM8","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["x_test[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QDFb1RGQ1n1","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["reloaded_predictor.predict(x_test[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2G18DO24TsIn"},"source":["# Text Classification with Universal Sentence Encoder on TensorFlow Hub\n","\n","この例では、[TensorFlow Hub](https://tfhub.dev/)を使って文書分類をしてみましょう。TensorFlow Hubとは何かというと、機械学習モデルのリポジトリです。より噛み砕いて言うと、画像分類やテキスト分類、音声認識といったタスクに使うことのできる学習済みのモデルが置いてある場所です。TensorFlow Hubから学習済みのモデルをダウンロードすることで、そのモデルを基に自分のモデルを構築することができます。\n","\n","今回は、文書分類のために、Multilingual Universal Sentence Encoder(m-USE)と呼ばれるモデルを利用することにしましょう。m-USEを使うことで、多言語の文表現を得られます。モデルの詳細は以下から確認することができます。\n","\n","- [Multilingual Universal Sentence Encoder | TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3)"]},{"cell_type":"markdown","metadata":{"id":"XqGV9OMYTydo"},"source":["## Setup\n","\n","m-USEは、入力の処理を[TensorFlow Text](https://github.com/tensorflow/text)に依存しています。そのため、事前にTensorFlow Textをインストールしておく必要があります。"]},{"cell_type":"code","metadata":{"id":"1lkkK2t5efGK","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["!pip install tensorflow-text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s35ZvCLeT79V"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"qYOf0IcGT30d","executionInfo":{"status":"aborted","timestamp":1628491796052,"user_tz":-540,"elapsed":486,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["import string\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_text as text\n","import tensorflow_hub as hub\n","from bs4 import BeautifulSoup\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import load_model, Model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqx1xSxOUAxB"},"source":["### Resource\n","\n","TensorFlow Hubを使って、m-USEをKerasのレイヤーとしてロードします。`trainable`パラメータによって、学習する際に重みを更新するか否かを決定します。今回はm-USEの重みも更新することにします。"]},{"cell_type":"code","metadata":{"id":"AI1gWP0XT-ZA","executionInfo":{"status":"aborted","timestamp":1628491796053,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["model_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3'\n","use_layer = hub.KerasLayer(model_url, trainable=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CAadnQTfUGZ1"},"source":["## The dataset"]},{"cell_type":"markdown","metadata":{"id":"k9XJ6RAMUIXI"},"source":["### Preprocess the dataset\n","\n","前処理としては以下の2つを行います。\n","- HTMLの除去\n","- テキストの切り詰め\n","\n","本来はトークン数でテキストを切り詰めるべきだと思いますが、今回は文字数で切り詰めてしまいます。m-USEのインターフェースの都合上、トークン数で切り詰めるのが容易ではなさそうなためです。"]},{"cell_type":"code","metadata":{"id":"jJxXS99lUDJb","executionInfo":{"status":"aborted","timestamp":1628491796053,"user_tz":-540,"elapsed":487,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["x = [clean_html(text) for text in x]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3inF4mutUNhK","executionInfo":{"status":"aborted","timestamp":1628491796054,"user_tz":-540,"elapsed":12,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xyMq-BiuUPvC","executionInfo":{"status":"aborted","timestamp":1628491796054,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["def create_input(input_strings, max_seq_length):\n","    input_texts = [text[:max_seq_length] for text in input_strings]\n","    return input_texts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yk0pMoLfURdy","executionInfo":{"status":"aborted","timestamp":1628491796054,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["max_seq_length = 300\n","train_inputs = create_input(x_train, max_seq_length=max_seq_length)\n","validation_inputs = create_input(x_test, max_seq_length=max_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJ6LPgndUTWJ"},"source":["## The model"]},{"cell_type":"markdown","metadata":{"id":"-S-tH8yiUVSK"},"source":["### Build the model"]},{"cell_type":"code","metadata":{"id":"m2mbxpaRUUW2","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":12,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["def get_model(use_layer, num_labels, rate=0.1):\n","    input_strings = tf.keras.layers.Input(shape=[], dtype=tf.string)\n","    pooled_output = use_layer(input_strings)\n","    pooled_output = tf.keras.layers.Dropout(rate=rate)(pooled_output)\n","    output = tf.keras.layers.Dense(units=num_labels, activation='softmax')(pooled_output)\n","\n","    return tf.keras.Model(\n","                inputs=[input_strings],\n","                outputs=output\n","            )\n","\n","\n","num_labels = 2\n","model = get_model(\n","    use_layer,\n","    num_labels=num_labels\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlI3TPGFUZZK","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":12,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMFWzIPWUeIx"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"fD_pqJDHUbX8","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["epochs = 100\n","batch_size = 16\n","save_path = '/tmp/model'\n","\n","model.compile(\n","    optimizer='sgd',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","model.fit(\n","      x=np.array(train_inputs), y=y_train,\n","      validation_split=0.2,\n","      epochs=epochs,\n","      callbacks=[\n","                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n","                 tf.keras.callbacks.ModelCheckpoint(\n","                     filepath=save_path,\n","                     monitor='val_loss',\n","                     save_best_only=True,\n","                     mode='min'\n","                 )\n","      ],\n","      shuffle=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYCETIT-Uhnh"},"source":["### Load the trained model"]},{"cell_type":"code","metadata":{"id":"a6eckcwqUfR8","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["model = load_model(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"feDC9xi7UlBI"},"source":["### Evaluate the model"]},{"cell_type":"code","metadata":{"id":"35kxg_2tUjjI","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["y_pred = model.predict(validation_inputs, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLSjU0JBUoyl","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":["print(classification_report(y_test, np.argmax(y_pred, axis=-1), digits=4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skeCI5M7UpFs","executionInfo":{"status":"aborted","timestamp":1628491796055,"user_tz":-540,"elapsed":11,"user":{"displayName":"大野有価","photoUrl":"","userId":"03079242618736976699"}}},"source":[""],"execution_count":null,"outputs":[]}]}