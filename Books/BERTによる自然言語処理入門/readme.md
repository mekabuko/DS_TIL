# メモ
## 1章 はじめに
### ニューラル言語モデル
自然言語処理の問題を解くニューラルネットワークのモデル。
- 文章からの特徴量抽出を自動的に行えるようになる。
- 文章や単語を「密なベクトル = ほとんどの要素が0ではないベクトル」に変換できる。
    - こうして文章や単語を密なベクトルとして表現したものを **分散表現** と呼ぶ
- 得られた分散表現は、何らかの形で文章やベクトルの意味を反映している
    - データの特徴料として使用することができる
### BERT
2018年にGoogleが発表したニューラル言語モデル。非常に高い性能を出した。
- 文脈を考慮した分散表現を生成できる
    - 同じ単語でも、文脈（周りの文章）によって分散表現は変わる
    - Attention を取り入れることで、離れた位置にある情報も取り入れられるようになった
- 学習の過程は事前学習とファインチューニングに分けられる
1. 事前学習
    - 大量のデータで汎用的な言語パターンを学習
    - 本書では、Wikipediaの全ての記事データを用いて学習させたものを使用する（東北大学の研究グループが公開しているもの）
    - 事前学習済みモデルは様々公開されているので、タスクに合わせた適切なモデルを選ぶことも必要
2. ファインチューニング
    - 比較的少数のラベル付きデータで、特定のタスクに特化するように学習
    - BERTと分類器などを組み合わせて、個別のタスクに特化したモデルを構築する
    - この時のBERTは事前学習済みのものを使う
## 2章 ニューラルネットワークを用いた自然言語処理
### トークン化
文章を適当な単位に分割すること。これを実現するツールを「トーカナイザ」と呼ぶ。分割によって得られた要素を「トークン」と呼ぶ。
### ニューラルネットワークへの入力の制約
NNに入力できる語彙は固定（トークンの集合として、IDを割り振った語彙のセット分しか入力できない）。
語彙にないものは、未知語としてUNKNOWNなどになってしまう。
なので、未知語を減らすことが重要であるが、反面、語彙数はそのままパラメータ数になってしまうので、バランスが大切。
### 単語分割
文章を意味の最小単位である単語に分割する方法。
- 日本語のように区切りが明確でない場合には MeCab や Sudachi, Juman などの形態素解析ツールが必要。
    - 形態素解析ツールでは品詞などの統語的情報を付与してくれる機能もあるが、NNモデルで使うのは単語分割機能のみ。
### 文字分割
１文字ずつに分割する方法。語彙数は減らせるが、トークン数が増えるので計算量は多くなる。文字系列の持つ意味を考慮しないといけない。
### サブワード分割
単語をさらに部分文字列に分割する方法。**日本語のBERTもこの方法を採用**している。
- （例）大阪タワー、東京タワー、大阪大学　→ 大阪、東京、##タワー、##大学 ("##" は単語の途中に現れる要素であることを示す記号)
- 未知語に強いと言う特徴がある
- 語彙を適切に設定していないと良い結果は出ない。
    - コーパスから語彙を作成するアルゴリズムとして「Byte Pair Encoding(BPE)」や「WordPiece」がある。
        - トークン数が平均的に小さくなるように指定されたサイズの語彙を自動生成
- 全体として、計算量と未知語への対応のバランスが取れたトークン化が可能。
### 言語モデル
文章の出現しやすさを確率によってモデル化するもの。
- トークン出現確率の同時確率、として文章の出現確率を表現することができ、あるトークンの出現確率は、それまでの文章に出現した語彙（トークン列）に依存する。なので、ある文章の出現確率とは、それまでに出現した語彙を前提条件とした条件付き確率と見なすことができる。
- それまでに出現した語彙のトークン列を「文脈」と呼ぶ。※ 場合によっては、文章以後に出現したものも含む場合や、そのテキスト全体を文脈と呼ぶこともある
- 分散表現が似ている = 似たような文脈で出現するトークンである、と言える
    - コサイン類似度（ベクトル内積）で類似度を測る
- 文脈上、同じような場所に現れる対義語（暑い and 寒い）や、文章として出てくることの少ない自明な常識（バナナは黄色い）などを捉えることが苦手。

### Word2Vec
単語に対して一意の（文脈非依存の）分散表現を学習するモデル。こういった、文脈非依存の分散表現を与えることを「単語埋め込み(Word Embedding) と呼ぶ。
- 文脈を意識しないので、文脈によって意味が変わる、と言う場合に対応できない
    - 彼は舞台の「上手」に立った
    - 彼は料理「上手」だ
        - この二つの文章における「上手」を区別できない
- 語順を考慮できない
    - A は B に告白した
    - B は A に告白した
        - この二つの文章に同じ分散表現が与えられる

### ELMo
文脈を意識した分散表現を与えられるモデル。
- 双方向 LSTM を活用。
## 3章 BERT 
### BEAT とは
- RNNベースのモデルだと(LSTMを使用しても)、処理が長くなると、離れたトークンの情報が失われていってしまうという欠点があった。
    - これだと、文脈を処理するのに十分ではない。
- BEAT では、文章全体のトークンから、どのトークンに注目すれば良いか、というのを算出する
    - Attention(注意機構) と呼ばれる
- Transformer というモデルで提案された Transformer Encoder と呼ばれる Attention を用いた NN を用いている。

### BEAT への入力形式
#### 1. トークン化
- 単一文章、ないし、タスクによっては文章のペア（質問＆回答、など）を入力できるようにする
- それぞれの文章をトークン化（形態素分割）する。
- トークン列の最初に特殊トークン `[CLS]`、末尾に `[SEP]`を追加する。
- 文章ペアの場合には、文章の境界に `[SEP]` を置く。
    - `[SEP]`: 文章の境界や、入力の終了を示す役割。
    - `[CLS]`: `[CLS]` に対応する　BERT の出力は、文章の分散表現として用いることができる。
#### 2. ベクトル化
- 各トークンをベクトルに置き換えて、BERT に入力する。
- 以下の3つのベクトルの和をBERTに入力。
    - トークン種別
    - 文章タイプ
    - 文章中の位置
### 3. BERT の学習
#### 事前学習
- ラベルなしの大量のデータで事前学習（環境によるが、数日から数週間かかる）
- 公開されているものもあるので、それを使うことも可能。
##### マスク付き言語モデル
- トークンの一定量（１５％）を `[MASK]` に置き換えて、元のトークンを予測する（トークン確率を求める）学習を行う。
##### Next Sequence Prediction
- 1/2 の確率で連続する文章のペアを入力として、連続する文章であるかどうかを判別する。
#### ファインチューニング
- BERT そのものは、文章の特徴量を抽出するためのエンコーダのような働きをする
- それを、分類機などに接続することで、個別のタスクに対応する
    - そのための、各タスクに特化するための学習がファインチューニング
## 4章 Huggingface Transformers
- [実装サンプル](./Chapter04.ipunb)
### 使用するライブラリ
- PyTorch: Facebook社製の深層学習フレームワーク
- Transformers: NN言語モデルライブラリ
    - Huggingface社が提供しているOSSのライブラリ。BERTをはじめとして様々なNN言語モデルが利用可能。
    - 様々な言語（日本語含む）の事前学習モデルが利用可能。
        - 日本語では、東北大学の研究チームによってWikipedia記事のデータをもとに作成されたもの、など。
- Fugashi: 日本語の形態素解析ツールのMeCab を Python から使えるようにしたもの
- ipadic: MeCab で形態素解析を行う際に用いる辞書
## 5章 文章の穴埋め
- [実装サンプル](./Chapter05.ipunb)
### 穴埋めに使用するBERT
- モデルは `cl-tohoku/bert-base-japanese-whole-word-masking`
- クラスは穴埋め用の `BertForMaskedLM`
### 穴埋めタスクの手順
1. まず、文章の一部を特殊トークン`[MASK]`に置き換えたものを用意。
2. それをトークン化
3. `BertForMaskedLM` で予測すると、`[MASK]` に入るトークンを語彙の中から予測する
### 穴が複数ある場合
- 貪欲法：まず、1つ目を一番スコアの高いやつで埋めて、2つ目を同様に予測、という手順で行う
    - ただし、BERT は文章生成のような、大量に穴が空いているものを連続で埋める、はあまり得意ではない
- ビームサーチ法
    - 穴埋めの際に候補を上位10個くらい持っておいて、組み合わせをある程度考慮する
## 6章 文章分類
- [実装サンプル](./Chapter06.ipunb)
### 使用するライブラリ
- PyTorch Lightning: ファインチューニングと性能評価を効率的に行うためのライブラリ
### 文章分類に使用するBERT
- モデルは `cl-tohoku/bert-base-japanese-whole-word-masking`
- クラスは穴埋め用の `BertForSequenceClassification`
### 文章分類とは
- その文章を与えられたカテゴリーに分類するタスク
    - （例）ネガポジ分析：文章がポジティブなものか、ネガティブなものかを分類する感情分析の一種
- 複数の分類に属することを許容する、マルチラベル分類も存在
### データローダ
- ファインチューニングや性能評価のために、データセットをBERTに入力可能な形式にする必要がある
- PyTorchでは「データローダ」という形式にする
    - `DataLoader` クラスを使用
    - イテレーティブで、ミニバッチの取り出しが可能
### 文章の符号化
- 文章を符号化(ID付番)して、入力できる形にする
- BERT は 最大で 512 トークンまでしか受け入れられないので注意
### PyTorch Lightning　を用いたファインチューニングとテスト
- チューニングした結果を、ファイルとして書き出しておくことができる
    - `config.json` と `pytorch_model.bin`
    - transformars に読み込める形式でも可能: `model.bert_sc.save_pretrained([path])`
    - 上記が入ったディレクトリを指定することで、モデルのロードも可能: `BertForSequenceClassification.from_pretrained([path])`
- 結果は、Tensorboard で確認可能

# Link
- [公式サポート](https://github.com/stockmarkteam/bert-book)