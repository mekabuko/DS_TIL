# メモ
## 1章 はじめに
### ニューラル言語モデル
自然言語処理の問題を解くニューラルネットワークのモデル。
- 文章からの特徴量抽出を自動的に行えるようになる。
- 文章や単語を「密なベクトル = ほとんどの要素が0ではないベクトル」に変換できる。
    - こうして文章や単語を密なベクトルとして表現したものを **分散表現** と呼ぶ
- 得られた分散表現は、何らかの形で文章やベクトルの意味を反映している
    - データの特徴料として使用することができる
### BERT
2018年にGoogleが発表したニューラル言語モデル。非常に高い性能を出した。
- 文脈を考慮した分散表現を生成できる
    - 同じ単語でも、文脈（周りの文章）によって分散表現は変わる
    - Attention を取り入れることで、離れた位置にある情報も取り入れられるようになった
- 学習の過程は事前学習とファインチューニングに分けられる
1. 事前学習
    - 大量のデータで汎用的な言語パターンを学習
    - 本書では、Wikipediaの全ての記事データを用いて学習させたものを使用する（東北大学の研究グループが公開しているもの）
    - 事前学習済みモデルは様々公開されているので、タスクに合わせた適切なモデルを選ぶことも必要
2. ファインチューニング
    - 比較的少数のラベル付きデータで、特定のタスクに特化するように学習
    - BERTと分類器などを組み合わせて、個別のタスクに特化したモデルを構築する
    - この時のBERTは事前学習済みのものを使う
## 2章 ニューラルネットワークを用いた自然言語処理
### トークン化
文章を適当な単位に分割すること。これを実現するツールを「トーカナイザ」と呼ぶ。分割によって得られた要素を「トークン」と呼ぶ。
### ニューラルネットワークへの入力の制約
NNに入力できる語彙は固定（トークンの集合として、IDを割り振った語彙のセット分しか入力できない）。
語彙にないものは、未知語としてUNKNOWNなどになってしまう。
なので、未知語を減らすことが重要であるが、反面、語彙数はそのままパラメータ数になってしまうので、バランスが大切。
### 単語分割
文章を意味の最小単位である単語に分割する方法。
- 日本語のように区切りが明確でない場合には MeCab や Sudachi, Juman などの形態素解析ツールが必要。
    - 形態素解析ツールでは品詞などの統語的情報を付与してくれる機能もあるが、NNモデルで使うのは単語分割機能のみ。
### 文字分割
１文字ずつに分割する方法。語彙数は減らせるが、トークン数が増えるので計算量は多くなる。文字系列の持つ意味を考慮しないといけない。
### サブワード分割
単語をさらに部分文字列に分割する方法。**日本語のBERTもこの方法を採用**している。
- （例）大阪タワー、東京タワー、大阪大学　→ 大阪、東京、##タワー、##大学 ("##" は単語の途中に現れる要素であることを示す記号)
- 未知語に強いと言う特徴がある
- 語彙を適切に設定していないと良い結果は出ない。
    - コーパスから語彙を作成するアルゴリズムとして「Byte Pair Encoding(BPE)」や「WordPiece」がある。
        - トークン数が平均的に小さくなるように指定されたサイズの語彙を自動生成
- 全体として、計算量と未知語への対応のバランスが取れたトークン化が可能。
### 言語モデル
文章の出現しやすさを確率によってモデル化するもの。
- トークン出現確率の同時確率、として文章の出現確率を表現することができ、あるトークンの出現確率は、それまでの文章に出現した語彙（トークン列）に依存する。なので、ある文章の出現確率とは、それまでに出現した語彙を前提条件とした条件付き確率と見なすことができる。
- それまでに出現した語彙のトークン列を「文脈」と呼ぶ。※ 場合によっては、文章以後に出現したものも含む場合や、そのテキスト全体を文脈と呼ぶこともある
- 分散表現が似ている = 似たような文脈で出現するトークンである、と言える
    - コサイン類似度（ベクトル内積）で類似度を測る
- 文脈上、同じような場所に現れる対義語（暑い and 寒い）や、文章として出てくることの少ない自明な常識（バナナは黄色い）などを捉えることが苦手。

### Word2Vec
単語に対して一意の（文脈非依存の）分散表現を学習するモデル。こういった、文脈非依存の分散表現を与えることを「単語埋め込み(Word Embedding) と呼ぶ。
- 文脈を意識しないので、文脈によって意味が変わる、と言う場合に対応できない
    - 彼は舞台の「上手」に立った
    - 彼は料理「上手」だ
        - この二つの文章における「上手」を区別できない
- 語順を考慮できない
    - A は B に告白した
    - B は A に告白した
        - この二つの文章に同じ分散表現が与えられる

### ELMo
文脈を意識した分散表現を与えられるモデル。
- 双方向 LSTM を活用。


# Link
- [公式サポート](https://github.com/stockmarkteam/bert-book)